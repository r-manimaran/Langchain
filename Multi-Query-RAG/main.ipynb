{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from Huggingface using datasets\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the document with metadata to be pushed to Vector index\n",
    "from langchain.docstore.document import Document\n",
    "docs =[]\n",
    "\n",
    "for row in ds:\n",
    "   doc = Document(\n",
    "       page_content=row['chunk'],\n",
    "       metadata={\n",
    "           'source': row['source'],\n",
    "           'title': row['title'],\n",
    "           'text': row['chunk'],\n",
    "           'chunk-id':row['chunk-id'],\n",
    "           'id':row['id']\n",
    "         })\n",
    "   docs.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "model_name =\"text-embedding-ada-002\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=model_name,disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "api_key = os.environ['PINECONE_API_KEY']\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'langchain-multi-query-demo' not already exists. creating now\n"
     ]
    }
   ],
   "source": [
    "# create the Index\n",
    "import time\n",
    "index_name =\"langchain-multi-query-demo\"\n",
    "existing_indexes =[ index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "#chck if the index exists\n",
    "if index_name not in existing_indexes:\n",
    "    print(f\"Index '{index_name}' not already exists. creating now\")\n",
    "    pc.create_index(index_name,\n",
    "                    dimension=1536,\n",
    "                    metric =\"dotproduct\",\n",
    "                    spec =spec\n",
    "                    )\n",
    "    #wait for index to be created\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "    # connect to index\n",
    "    index = pc.Index(index_name)\n",
    "    time.sleep(1)\n",
    "    #view index status\n",
    "    index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41584"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [25:17<00:00,  7.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# create batch and upload the data to pinecone index\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "batch_size = 200\n",
    "\n",
    "for i in tqdm(range(0,len(docs),batch_size)):\n",
    "    i_end =min(len(docs), i+batch_size)\n",
    "    docs_batch = docs[i:i_end]\n",
    "    #get the ids\n",
    "    ids =[f\"{doc.metadata['id']} - {doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
    "    # get text and embeddings\n",
    "    texts = [doc.page_content for doc in docs_batch]\n",
    "    embeds =embeddings.embed_documents(texts)\n",
    "    #get metadata\n",
    "    metadata = [doc.metadata for doc in docs_batch]\n",
    "    to_upsert = zip(ids, embeds, metadata)\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "text_field = \"text\"\n",
    "vectorstore = Pinecone(index, embeddings.embed_query, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    retriever= vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 16:28:52,775 - DEBUG- Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question:  Tell me about llama2?', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'logprobs': False, 'n': 1, 'stream': False, 'temperature': 0.0}}\n",
      " 2024-08-02 16:28:52,778 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      " 2024-08-02 16:28:52,780 - DEBUG- close.started\n",
      " 2024-08-02 16:28:52,783 - DEBUG- close.complete\n",
      " 2024-08-02 16:28:52,784 - DEBUG- connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 16:28:52,956 - DEBUG- connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C4CCB5700>\n",
      " 2024-08-02 16:28:52,958 - DEBUG- start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023C639218D0> server_hostname='api.openai.com' timeout=None\n",
      " 2024-08-02 16:28:53,094 - DEBUG- start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C4CCB5B20>\n",
      " 2024-08-02 16:28:53,095 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:53,098 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 16:28:53,099 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:53,100 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 16:28:53,102 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:53,965 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 20:28:54 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'586'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199867'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_1a96e7208fb46a1f77755005c82574e2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad0d120ff61801d-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 16:28:53,967 - INFO- HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 16:28:53,968 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:54,020 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 16:28:54,022 - DEBUG- response_closed.started\n",
      " 2024-08-02 16:28:54,023 - DEBUG- response_closed.complete\n",
      " 2024-08-02 16:28:54,025 - DEBUG- HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 20:28:54 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '586', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199867', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '39ms', 'x-request-id': 'req_1a96e7208fb46a1f77755005c82574e2', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad0d120ff61801d-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 16:28:54,026 - DEBUG- request_id: req_1a96e7208fb46a1f77755005c82574e2\n",
      " 2024-08-02 16:28:54,031 - INFO- Generated queries: ['What information can you provide about llama2?', 'What are some details about llama2?', 'Can you share some insights on llama2?']\n",
      " 2024-08-02 16:28:54,046 - DEBUG- Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000023C63F9AFC0>, 'json_data': {'input': [[3923, 2038, 649, 499, 3493, 922, 94776, 17, 30]], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      " 2024-08-02 16:28:54,050 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      " 2024-08-02 16:28:54,053 - DEBUG- close.started\n",
      " 2024-08-02 16:28:54,057 - DEBUG- close.complete\n",
      " 2024-08-02 16:28:54,058 - DEBUG- connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      " 2024-08-02 16:28:54,077 - DEBUG- connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C436EFC80>\n",
      " 2024-08-02 16:28:54,078 - DEBUG- start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023C6214E2D0> server_hostname='api.openai.com' timeout=None\n",
      " 2024-08-02 16:28:54,115 - DEBUG- start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C42FFC5F0>\n",
      " 2024-08-02 16:28:54,117 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:54,119 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 16:28:54,121 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:54,123 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 16:28:54,125 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:54,268 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 20:28:54 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999990'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_97b4b8e9e19f4e38c1d14cc492c28eca'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad0d12758ba2431-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 16:28:54,270 - INFO- HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 16:28:54,271 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:54,304 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 16:28:54,306 - DEBUG- response_closed.started\n",
      " 2024-08-02 16:28:54,308 - DEBUG- response_closed.complete\n",
      " 2024-08-02 16:28:54,309 - DEBUG- HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 20:28:54 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '27', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999990', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_97b4b8e9e19f4e38c1d14cc492c28eca', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad0d12758ba2431-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 16:28:54,310 - DEBUG- request_id: req_97b4b8e9e19f4e38c1d14cc492c28eca\n",
      " 2024-08-02 16:28:55,590 - DEBUG- response body: b'{\"results\":[],\"matches\":[{\"id\":\"2307.09288 - 1\",\"score\":0.831083536,\"values\":[],\"metadata\":{\"chunk-id\":\"1\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\\\nSergey Edunov Thomas Scialom\\\\u0003\\\\nGenAI, Meta\\\\nAbstract\\\\nIn this work, we develop and release Llama 2, a collection of pretrained and \\xef\\xac\\x81ne-tuned\\\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\\\nOur \\xef\\xac\\x81ne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to \\xef\\xac\\x81ne-tuning and safety\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2307.09288 - 9\",\"score\":0.824650109,\"values\":[],\"metadata\":{\"chunk-id\":\"9\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\\xef\\xac\\x81ne-tunedtoalignwithhuman\\\\npreferences, which greatly enhances their usability and safety. This step can require signi\\xef\\xac\\x81cant costs in\\\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\\\nthe community to advance AI alignment research.\\\\nIn this work, we develop and release Llama 2, a family of pretrained and \\xef\\xac\\x81ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2304.01196 - 37\",\"score\":0.795696914,\"values\":[],\"metadata\":{\"chunk-id\":\"37\",\"id\":\"2304.01196\",\"source\":\"http://arxiv.org/pdf/2304.01196\",\"text\":\"Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\\\nAn instruction-following llama model. https://\\\\ngithub.com/tatsu-lab/stanford_alpaca .\\\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\\\nMartinet, Marie-Anne Lachaux, Timoth\\xc3\\xa9e Lacroix,\\\\nBaptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric Hambro, Faisal\\\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\\\nGrave, and Guillaume Lample. 2023. Llama: Open\\\\nand efficient foundation language models. arXiv\\\\npreprint arXiv:2302.13971 .\",\"title\":\"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data\"}},{\"id\":\"2307.09288 - 199\",\"score\":0.795691788,\"values\":[],\"metadata\":{\"chunk-id\":\"199\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"Ricardo Lopez-Barquilla, Marc Shedro\\xef\\xac\\x80, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\\\n\\xe2\\x80\\xa2ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\\\n46\\\\n\\xe2\\x80\\xa2Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\\\nLlama team who helped get this work started.\\\\n\\xe2\\x80\\xa2Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the \\xef\\xac\\x81gures in the\\\\npaper.\\\\n\\xe2\\x80\\xa2Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\\\ninternal demo.\\\\n\\xe2\\x80\\xa2Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\\\nLaurens van der Maaten, Jason Weston, and Omer Levy.\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}}],\"namespace\":\"\",\"usage\":{\"readUnits\":6}}'\n",
      " 2024-08-02 16:28:55,605 - DEBUG- Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000023C5545E660>, 'json_data': {'input': [[3923, 527, 1063, 3649, 922, 94776, 17, 30]], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      " 2024-08-02 16:28:55,607 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      " 2024-08-02 16:28:55,608 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:55,610 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 16:28:55,611 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:55,615 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 16:28:55,616 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:55,775 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 20:28:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999992'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_aaa5bf043dd7e8922e2c92bdfbe93c8d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad0d130aa472431-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 16:28:55,776 - INFO- HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 16:28:55,777 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:55,785 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 16:28:55,786 - DEBUG- response_closed.started\n",
      " 2024-08-02 16:28:55,788 - DEBUG- response_closed.complete\n",
      " 2024-08-02 16:28:55,791 - DEBUG- HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 20:28:55 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '34', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999992', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_aaa5bf043dd7e8922e2c92bdfbe93c8d', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad0d130aa472431-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 16:28:55,793 - DEBUG- request_id: req_aaa5bf043dd7e8922e2c92bdfbe93c8d\n",
      " 2024-08-02 16:28:55,893 - DEBUG- response body: b'{\"results\":[],\"matches\":[{\"id\":\"2307.09288 - 1\",\"score\":0.849314094,\"values\":[],\"metadata\":{\"chunk-id\":\"1\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\\\nSergey Edunov Thomas Scialom\\\\u0003\\\\nGenAI, Meta\\\\nAbstract\\\\nIn this work, we develop and release Llama 2, a collection of pretrained and \\xef\\xac\\x81ne-tuned\\\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\\\nOur \\xef\\xac\\x81ne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to \\xef\\xac\\x81ne-tuning and safety\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2307.09288 - 9\",\"score\":0.829001307,\"values\":[],\"metadata\":{\"chunk-id\":\"9\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\\xef\\xac\\x81ne-tunedtoalignwithhuman\\\\npreferences, which greatly enhances their usability and safety. This step can require signi\\xef\\xac\\x81cant costs in\\\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\\\nthe community to advance AI alignment research.\\\\nIn this work, we develop and release Llama 2, a family of pretrained and \\xef\\xac\\x81ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2304.01196 - 37\",\"score\":0.803102553,\"values\":[],\"metadata\":{\"chunk-id\":\"37\",\"id\":\"2304.01196\",\"source\":\"http://arxiv.org/pdf/2304.01196\",\"text\":\"Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\\\nAn instruction-following llama model. https://\\\\ngithub.com/tatsu-lab/stanford_alpaca .\\\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\\\nMartinet, Marie-Anne Lachaux, Timoth\\xc3\\xa9e Lacroix,\\\\nBaptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric Hambro, Faisal\\\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\\\nGrave, and Guillaume Lample. 2023. Llama: Open\\\\nand efficient foundation language models. arXiv\\\\npreprint arXiv:2302.13971 .\",\"title\":\"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data\"}},{\"id\":\"2307.09288 - 317\",\"score\":0.801270604,\"values\":[],\"metadata\":{\"chunk-id\":\"317\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"models will be released as we improve model safety with community feedback.\\\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\\\nmodels-and-libraries/llama-downloads/\\\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\\\nfound in the model README, or by opening an issue in the GitHub repository\\\\n(https://github.com/facebookresearch/llama/ ).\\\\nIntended Use\\\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\\\nfor a variety of natural language generation tasks.\\\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\\\nHardware and Software (Section 2.2)\\\\nTraining Factors We usedcustomtraininglibraries, Meta\\xe2\\x80\\x99sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}}],\"namespace\":\"\",\"usage\":{\"readUnits\":6}}'\n",
      " 2024-08-02 16:28:55,903 - DEBUG- Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000023C5545EB60>, 'json_data': {'input': [[6854, 499, 4430, 1063, 26793, 389, 94776, 17, 30]], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      " 2024-08-02 16:28:55,906 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      " 2024-08-02 16:28:55,909 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:55,911 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 16:28:55,912 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:55,915 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 16:28:55,916 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:56,056 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 20:28:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'20'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999990'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_35b3da70ebbe4c70a508b586d8f9e1ea'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad0d1328c2f2431-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 16:28:56,058 - INFO- HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 16:28:56,059 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:28:56,084 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 16:28:56,085 - DEBUG- response_closed.started\n",
      " 2024-08-02 16:28:56,086 - DEBUG- response_closed.complete\n",
      " 2024-08-02 16:28:56,089 - DEBUG- HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 20:28:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '20', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999990', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_35b3da70ebbe4c70a508b586d8f9e1ea', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad0d1328c2f2431-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 16:28:56,090 - DEBUG- request_id: req_35b3da70ebbe4c70a508b586d8f9e1ea\n",
      " 2024-08-02 16:28:56,198 - DEBUG- response body: b'{\"results\":[],\"matches\":[{\"id\":\"2307.09288 - 1\",\"score\":0.843613207,\"values\":[],\"metadata\":{\"chunk-id\":\"1\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\\\nSergey Edunov Thomas Scialom\\\\u0003\\\\nGenAI, Meta\\\\nAbstract\\\\nIn this work, we develop and release Llama 2, a collection of pretrained and \\xef\\xac\\x81ne-tuned\\\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\\\nOur \\xef\\xac\\x81ne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to \\xef\\xac\\x81ne-tuning and safety\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2307.09288 - 9\",\"score\":0.83098954,\"values\":[],\"metadata\":{\"chunk-id\":\"9\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\\xef\\xac\\x81ne-tunedtoalignwithhuman\\\\npreferences, which greatly enhances their usability and safety. This step can require signi\\xef\\xac\\x81cant costs in\\\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\\\nthe community to advance AI alignment research.\\\\nIn this work, we develop and release Llama 2, a family of pretrained and \\xef\\xac\\x81ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2304.01196 - 37\",\"score\":0.813130796,\"values\":[],\"metadata\":{\"chunk-id\":\"37\",\"id\":\"2304.01196\",\"source\":\"http://arxiv.org/pdf/2304.01196\",\"text\":\"Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\\\nAn instruction-following llama model. https://\\\\ngithub.com/tatsu-lab/stanford_alpaca .\\\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\\\nMartinet, Marie-Anne Lachaux, Timoth\\xc3\\xa9e Lacroix,\\\\nBaptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric Hambro, Faisal\\\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\\\nGrave, and Guillaume Lample. 2023. Llama: Open\\\\nand efficient foundation language models. arXiv\\\\npreprint arXiv:2302.13971 .\",\"title\":\"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data\"}},{\"id\":\"2307.09288 - 317\",\"score\":0.804392576,\"values\":[],\"metadata\":{\"chunk-id\":\"317\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"models will be released as we improve model safety with community feedback.\\\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\\\nmodels-and-libraries/llama-downloads/\\\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\\\nfound in the model README, or by opening an issue in the GitHub repository\\\\n(https://github.com/facebookresearch/llama/ ).\\\\nIntended Use\\\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\\\nfor a variety of natural language generation tasks.\\\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\\\nHardware and Software (Section 2.2)\\\\nTraining Factors We usedcustomtraininglibraries, Meta\\xe2\\x80\\x99sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}}],\"namespace\":\"\",\"usage\":{\"readUnits\":6}}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question =\" Tell me about llama2?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Generation in RAG\n",
    "- Pass the retrieved Docs from vector store to LLM and get the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"contexts\"],\n",
    "    template=\"\"\" You are a helpful assistant who answers user queries using the contexts provided. If the question\n",
    "                cannot be answered using the contexts, you will say \"I don't know\".\n",
    "                Contexts: {contexts} \n",
    "\n",
    "                Question: {question} \"\"\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 16:48:14,698 - DEBUG- Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': ' You are a helpful assistant who answers user queries using the contexts provided. If the question\\n                cannot be answered using the contexts, you will say \"I don\\'t know\".\\n                Contexts: Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\n---\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\n---\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 .\\n---\\nRicardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.\\n---\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso \\n\\n                Question:  Tell me about llama2? ', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'logprobs': False, 'n': 1, 'stream': False, 'temperature': 0.0}}\n",
      " 2024-08-02 16:48:14,701 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      " 2024-08-02 16:48:14,705 - DEBUG- close.started\n",
      " 2024-08-02 16:48:14,707 - DEBUG- close.complete\n",
      " 2024-08-02 16:48:14,708 - DEBUG- connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 16:48:14,811 - DEBUG- connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C4313B200>\n",
      " 2024-08-02 16:48:14,813 - DEBUG- start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023C639218D0> server_hostname='api.openai.com' timeout=None\n",
      " 2024-08-02 16:48:14,857 - DEBUG- start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C576D30B0>\n",
      " 2024-08-02 16:48:14,859 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:48:14,861 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 16:48:14,862 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:48:14,865 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 16:48:14,866 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:48:16,535 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 20:48:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'1424'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'198771'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'368ms'), (b'x-request-id', b'req_63a6e54f039cfabb075cb21fccd48f7e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Zppf_i3KfWuqDikc3w..aKDa.N937quXhLFHBqf.tbw-1722631696-1.0.1.1-AK8NAReVOCCFYnmRIbHL6yW5i7g_ShRLJIreQ5UtiPrBuCDWAXfzrVbCD.k_FFynepIkuhKzn7Yz962cDaHYGQ; path=/; expires=Fri, 02-Aug-24 21:18:16 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad0ed7e0fad53d2-ATL'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 16:48:16,536 - INFO- HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 16:48:16,537 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 16:48:16,597 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 16:48:16,599 - DEBUG- response_closed.started\n",
      " 2024-08-02 16:48:16,600 - DEBUG- response_closed.complete\n",
      " 2024-08-02 16:48:16,603 - DEBUG- HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 20:48:16 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '1424', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '198771', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '368ms', 'x-request-id': 'req_63a6e54f039cfabb075cb21fccd48f7e', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=Zppf_i3KfWuqDikc3w..aKDa.N937quXhLFHBqf.tbw-1722631696-1.0.1.1-AK8NAReVOCCFYnmRIbHL6yW5i7g_ShRLJIreQ5UtiPrBuCDWAXfzrVbCD.k_FFynepIkuhKzn7Yz962cDaHYGQ; path=/; expires=Fri, 02-Aug-24 21:18:16 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad0ed7e0fad53d2-ATL', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 16:48:16,606 - DEBUG- request_id: req_63a6e54f039cfabb075cb21fccd48f7e\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = chain(inputs={\n",
    "    \"question\":question,\n",
    "    \"contexts\":\"\\n---\\n\".join([d.page_content for d in docs])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ' Tell me about llama2?',\n",
       " 'contexts': 'Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\n---\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\n---\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 .\\n---\\nRicardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.\\n---\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso',\n",
       " 'text': 'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. These models outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models. The development and release of Llama 2 aim to advance AI alignment research within the community.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining everything in Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "def retrieval_transform(inputs:dict)->dict:\n",
    "    docs = retriever.get_relevant_documents(inputs['question'])\n",
    "    docs = [doc.page_content for doc in docs]\n",
    "    docs_dict ={\n",
    "        \"query\":inputs[\"question\"],\n",
    "        \"contexts\":\"\\n--\\n\".join(docs)\n",
    "    }\n",
    "    return docs_dict\n",
    "    \n",
    "retreival_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\",\"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retreival_chain, chain],\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\",\"contexts\",\"text\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 17:15:23,728 - DEBUG- Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question:  Tell me about llama2?', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'logprobs': False, 'n': 1, 'stream': False, 'temperature': 0.0}}\n",
      " 2024-08-02 17:15:23,731 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      " 2024-08-02 17:15:23,734 - DEBUG- close.started\n",
      " 2024-08-02 17:15:23,736 - DEBUG- close.complete\n",
      " 2024-08-02 17:15:23,737 - DEBUG- connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 17:15:23,823 - DEBUG- connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C63D86180>\n",
      " 2024-08-02 17:15:23,824 - DEBUG- start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023C639218D0> server_hostname='api.openai.com' timeout=None\n",
      " 2024-08-02 17:15:23,866 - DEBUG- start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C63D86240>\n",
      " 2024-08-02 17:15:23,868 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:23,871 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 17:15:23,875 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:23,876 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 17:15:23,878 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 17:15:24,694 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 21:15:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'576'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199867'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-request-id', b'req_86d773b74a33a7a1a87745f429ff375c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad1154368680588-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 17:15:24,695 - INFO- HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 17:15:24,697 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:24,698 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 17:15:24,700 - DEBUG- response_closed.started\n",
      " 2024-08-02 17:15:24,701 - DEBUG- response_closed.complete\n",
      " 2024-08-02 17:15:24,703 - DEBUG- HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 21:15:24 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '576', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199867', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '39ms', 'x-request-id': 'req_86d773b74a33a7a1a87745f429ff375c', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad1154368680588-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 17:15:24,704 - DEBUG- request_id: req_86d773b74a33a7a1a87745f429ff375c\n",
      " 2024-08-02 17:15:24,712 - INFO- Generated queries: ['What information can you provide about llama2?', 'What are some details about llama2?', 'Can you share some insights on llama2?']\n",
      " 2024-08-02 17:15:24,729 - DEBUG- Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000023C652B9DA0>, 'json_data': {'input': [[3923, 2038, 649, 499, 3493, 922, 94776, 17, 30]], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      " 2024-08-02 17:15:24,733 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      " 2024-08-02 17:15:24,734 - DEBUG- close.started\n",
      " 2024-08-02 17:15:24,737 - DEBUG- close.complete\n",
      " 2024-08-02 17:15:24,739 - DEBUG- connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      " 2024-08-02 17:15:24,766 - DEBUG- connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C576D3980>\n",
      " 2024-08-02 17:15:24,767 - DEBUG- start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023C6214E2D0> server_hostname='api.openai.com' timeout=None\n",
      " 2024-08-02 17:15:24,828 - DEBUG- start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C576D30B0>\n",
      " 2024-08-02 17:15:24,829 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:24,832 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 17:15:24,834 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:24,835 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 17:15:24,840 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:24,990 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 21:15:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'28'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999990'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_c10870953f20cd87f10a2ab7a64715f3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad115496a7581b5-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 17:15:24,992 - INFO- HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 17:15:24,993 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:24,996 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 17:15:24,997 - DEBUG- response_closed.started\n",
      " 2024-08-02 17:15:24,998 - DEBUG- response_closed.complete\n",
      " 2024-08-02 17:15:25,000 - DEBUG- HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 21:15:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '28', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999990', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_c10870953f20cd87f10a2ab7a64715f3', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad115496a7581b5-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 17:15:25,001 - DEBUG- request_id: req_c10870953f20cd87f10a2ab7a64715f3\n",
      " 2024-08-02 17:15:25,095 - DEBUG- response body: b'{\"results\":[],\"matches\":[{\"id\":\"2307.09288 - 1\",\"score\":0.831083536,\"values\":[],\"metadata\":{\"chunk-id\":\"1\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\\\nSergey Edunov Thomas Scialom\\\\u0003\\\\nGenAI, Meta\\\\nAbstract\\\\nIn this work, we develop and release Llama 2, a collection of pretrained and \\xef\\xac\\x81ne-tuned\\\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\\\nOur \\xef\\xac\\x81ne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to \\xef\\xac\\x81ne-tuning and safety\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2307.09288 - 9\",\"score\":0.824650109,\"values\":[],\"metadata\":{\"chunk-id\":\"9\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\\xef\\xac\\x81ne-tunedtoalignwithhuman\\\\npreferences, which greatly enhances their usability and safety. This step can require signi\\xef\\xac\\x81cant costs in\\\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\\\nthe community to advance AI alignment research.\\\\nIn this work, we develop and release Llama 2, a family of pretrained and \\xef\\xac\\x81ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2304.01196 - 37\",\"score\":0.795696914,\"values\":[],\"metadata\":{\"chunk-id\":\"37\",\"id\":\"2304.01196\",\"source\":\"http://arxiv.org/pdf/2304.01196\",\"text\":\"Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\\\nAn instruction-following llama model. https://\\\\ngithub.com/tatsu-lab/stanford_alpaca .\\\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\\\nMartinet, Marie-Anne Lachaux, Timoth\\xc3\\xa9e Lacroix,\\\\nBaptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric Hambro, Faisal\\\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\\\nGrave, and Guillaume Lample. 2023. Llama: Open\\\\nand efficient foundation language models. arXiv\\\\npreprint arXiv:2302.13971 .\",\"title\":\"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data\"}},{\"id\":\"2307.09288 - 199\",\"score\":0.795691788,\"values\":[],\"metadata\":{\"chunk-id\":\"199\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"Ricardo Lopez-Barquilla, Marc Shedro\\xef\\xac\\x80, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\\\n\\xe2\\x80\\xa2ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\\\n46\\\\n\\xe2\\x80\\xa2Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\\\nLlama team who helped get this work started.\\\\n\\xe2\\x80\\xa2Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the \\xef\\xac\\x81gures in the\\\\npaper.\\\\n\\xe2\\x80\\xa2Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\\\ninternal demo.\\\\n\\xe2\\x80\\xa2Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\\\nLaurens van der Maaten, Jason Weston, and Omer Levy.\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}}],\"namespace\":\"\",\"usage\":{\"readUnits\":6}}'\n",
      " 2024-08-02 17:15:25,108 - DEBUG- Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000023C652BA660>, 'json_data': {'input': [[3923, 527, 1063, 3649, 922, 94776, 17, 30]], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      " 2024-08-02 17:15:25,109 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      " 2024-08-02 17:15:25,111 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,113 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 17:15:25,115 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,117 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 17:15:25,118 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,267 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 21:15:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'24'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999992'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_5244493ac327f81897585f4ac3bb9173'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad1154b3c4581b5-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 17:15:25,268 - INFO- HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 17:15:25,269 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,272 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 17:15:25,274 - DEBUG- response_closed.started\n",
      " 2024-08-02 17:15:25,275 - DEBUG- response_closed.complete\n",
      " 2024-08-02 17:15:25,277 - DEBUG- HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 21:15:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '24', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999992', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_5244493ac327f81897585f4ac3bb9173', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad1154b3c4581b5-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 17:15:25,279 - DEBUG- request_id: req_5244493ac327f81897585f4ac3bb9173\n",
      " 2024-08-02 17:15:25,324 - DEBUG- response body: b'{\"results\":[],\"matches\":[{\"id\":\"2307.09288 - 1\",\"score\":0.849571,\"values\":[],\"metadata\":{\"chunk-id\":\"1\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\\\nSergey Edunov Thomas Scialom\\\\u0003\\\\nGenAI, Meta\\\\nAbstract\\\\nIn this work, we develop and release Llama 2, a collection of pretrained and \\xef\\xac\\x81ne-tuned\\\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\\\nOur \\xef\\xac\\x81ne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to \\xef\\xac\\x81ne-tuning and safety\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2307.09288 - 9\",\"score\":0.829108357,\"values\":[],\"metadata\":{\"chunk-id\":\"9\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\\xef\\xac\\x81ne-tunedtoalignwithhuman\\\\npreferences, which greatly enhances their usability and safety. This step can require signi\\xef\\xac\\x81cant costs in\\\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\\\nthe community to advance AI alignment research.\\\\nIn this work, we develop and release Llama 2, a family of pretrained and \\xef\\xac\\x81ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2304.01196 - 37\",\"score\":0.80301708,\"values\":[],\"metadata\":{\"chunk-id\":\"37\",\"id\":\"2304.01196\",\"source\":\"http://arxiv.org/pdf/2304.01196\",\"text\":\"Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\\\nAn instruction-following llama model. https://\\\\ngithub.com/tatsu-lab/stanford_alpaca .\\\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\\\nMartinet, Marie-Anne Lachaux, Timoth\\xc3\\xa9e Lacroix,\\\\nBaptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric Hambro, Faisal\\\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\\\nGrave, and Guillaume Lample. 2023. Llama: Open\\\\nand efficient foundation language models. arXiv\\\\npreprint arXiv:2302.13971 .\",\"title\":\"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data\"}},{\"id\":\"2307.09288 - 317\",\"score\":0.801184058,\"values\":[],\"metadata\":{\"chunk-id\":\"317\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"models will be released as we improve model safety with community feedback.\\\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\\\nmodels-and-libraries/llama-downloads/\\\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\\\nfound in the model README, or by opening an issue in the GitHub repository\\\\n(https://github.com/facebookresearch/llama/ ).\\\\nIntended Use\\\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\\\nfor a variety of natural language generation tasks.\\\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\\\nHardware and Software (Section 2.2)\\\\nTraining Factors We usedcustomtraininglibraries, Meta\\xe2\\x80\\x99sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}}],\"namespace\":\"\",\"usage\":{\"readUnits\":6}}'\n",
      " 2024-08-02 17:15:25,336 - DEBUG- Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000023C652BAFC0>, 'json_data': {'input': [[6854, 499, 4430, 1063, 26793, 389, 94776, 17, 30]], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      " 2024-08-02 17:15:25,338 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      " 2024-08-02 17:15:25,339 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,342 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 17:15:25,343 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,345 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 17:15:25,347 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,493 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 21:15:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'23'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'1000000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'999990'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_d5d6052b2bba9d96e43b1e5d8cfcacc2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad1154c9daa81b5-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 17:15:25,495 - INFO- HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 17:15:25,496 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,500 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 17:15:25,501 - DEBUG- response_closed.started\n",
      " 2024-08-02 17:15:25,502 - DEBUG- response_closed.complete\n",
      " 2024-08-02 17:15:25,504 - DEBUG- HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 21:15:25 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '23', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999990', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_d5d6052b2bba9d96e43b1e5d8cfcacc2', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad1154c9daa81b5-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 17:15:25,506 - DEBUG- request_id: req_d5d6052b2bba9d96e43b1e5d8cfcacc2\n",
      " 2024-08-02 17:15:25,552 - DEBUG- response body: b'{\"results\":[],\"matches\":[{\"id\":\"2307.09288 - 1\",\"score\":0.843613207,\"values\":[],\"metadata\":{\"chunk-id\":\"1\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\\\nSergey Edunov Thomas Scialom\\\\u0003\\\\nGenAI, Meta\\\\nAbstract\\\\nIn this work, we develop and release Llama 2, a collection of pretrained and \\xef\\xac\\x81ne-tuned\\\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\\\nOur \\xef\\xac\\x81ne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to \\xef\\xac\\x81ne-tuning and safety\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2307.09288 - 9\",\"score\":0.83098954,\"values\":[],\"metadata\":{\"chunk-id\":\"9\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\\xef\\xac\\x81ne-tunedtoalignwithhuman\\\\npreferences, which greatly enhances their usability and safety. This step can require signi\\xef\\xac\\x81cant costs in\\\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\\\nthe community to advance AI alignment research.\\\\nIn this work, we develop and release Llama 2, a family of pretrained and \\xef\\xac\\x81ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}},{\"id\":\"2304.01196 - 37\",\"score\":0.813130796,\"values\":[],\"metadata\":{\"chunk-id\":\"37\",\"id\":\"2304.01196\",\"source\":\"http://arxiv.org/pdf/2304.01196\",\"text\":\"Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\\\nAn instruction-following llama model. https://\\\\ngithub.com/tatsu-lab/stanford_alpaca .\\\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\\\nMartinet, Marie-Anne Lachaux, Timoth\\xc3\\xa9e Lacroix,\\\\nBaptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric Hambro, Faisal\\\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\\\nGrave, and Guillaume Lample. 2023. Llama: Open\\\\nand efficient foundation language models. arXiv\\\\npreprint arXiv:2302.13971 .\",\"title\":\"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data\"}},{\"id\":\"2307.09288 - 317\",\"score\":0.804392576,\"values\":[],\"metadata\":{\"chunk-id\":\"317\",\"id\":\"2307.09288\",\"source\":\"http://arxiv.org/pdf/2307.09288\",\"text\":\"models will be released as we improve model safety with community feedback.\\\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\\\nmodels-and-libraries/llama-downloads/\\\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\\\nfound in the model README, or by opening an issue in the GitHub repository\\\\n(https://github.com/facebookresearch/llama/ ).\\\\nIntended Use\\\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\\\nfor a variety of natural language generation tasks.\\\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\\\nHardware and Software (Section 2.2)\\\\nTraining Factors We usedcustomtraininglibraries, Meta\\xe2\\x80\\x99sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\",\"title\":\"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}}],\"namespace\":\"\",\"usage\":{\"readUnits\":6}}'\n",
      " 2024-08-02 17:15:25,580 - DEBUG- Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': ' You are a helpful assistant who answers user queries using the contexts provided. If the question\\n                cannot be answered using the contexts, you will say \"I don\\'t know\".\\n                Contexts: Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\n--\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\n--\\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 .\\n--\\nRicardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.\\n--\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso \\n\\n                Question:  Tell me about llama2? ', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'logprobs': False, 'n': 1, 'stream': False, 'temperature': 0.0}}\n",
      " 2024-08-02 17:15:25,583 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      " 2024-08-02 17:15:25,585 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,586 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 17:15:25,588 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:25,589 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 17:15:25,592 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:27,193 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 21:15:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'1359'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'198771'), (b'x-ratelimit-reset-requests', b'15.563s'), (b'x-ratelimit-reset-tokens', b'368ms'), (b'x-request-id', b'req_416793573a67f8bc6482105cc3835e2c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad1154e2bc50588-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 17:15:27,194 - INFO- HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 17:15:27,195 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 17:15:27,197 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 17:15:27,200 - DEBUG- response_closed.started\n",
      " 2024-08-02 17:15:27,202 - DEBUG- response_closed.complete\n",
      " 2024-08-02 17:15:27,203 - DEBUG- HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 21:15:27 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '1359', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '198771', 'x-ratelimit-reset-requests': '15.563s', 'x-ratelimit-reset-tokens': '368ms', 'x-request-id': 'req_416793573a67f8bc6482105cc3835e2c', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad1154e2bc50588-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 17:15:27,204 - DEBUG- request_id: req_416793573a67f8bc6482105cc3835e2c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases and outperform open-source chat models on most benchmarks tested. They are intended for commercial and research use in English for assistant-like chat applications. More information can be found in the provided contexts.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rag_chain({\"question\":question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom MultiQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "#output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    lines: List[str] = Field(description=\"List of text\")\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self)->None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "    \n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "    \n",
    "output_parser = LineListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 18:22:19,856 - DEBUG- load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      " 2024-08-02 18:22:19,858 - DEBUG- load_verify_locations cafile='c:\\\\Maran\\\\StudyMaterials\\\\Git\\\\LangChain\\\\Multi-Query-RAG\\\\venv\\\\Lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      " 2024-08-02 18:22:21,100 - DEBUG- load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      " 2024-08-02 18:22:21,101 - DEBUG- load_verify_locations cafile='c:\\\\Maran\\\\StudyMaterials\\\\Git\\\\LangChain\\\\Multi-Query-RAG\\\\venv\\\\Lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "template =\"\"\"Your task is to generate 3 different search queries that aim to answer the user question from multiple perspetives.\n",
    "            Each query MUST tackle the question from a different perspective. \n",
    "            Provide these alternative questions separated by newlines.\n",
    "            \n",
    "            Original question: {question}\n",
    "            \"\"\"\n",
    "Query_Prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "llm_chain = LLMChain(llm=llm, prompt=Query_Prompt, output_parser= output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 18:37:19,851 - DEBUG- Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Your task is to generate 3 different search queries that aim to answer the user question from multiple perspetives.\\n            Each query MUST tackle the question from a different perspective. \\n            Provide these alternative questions separated by newlines.\\n            \\n            Original question: What is llama2?\\n            ', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'logprobs': False, 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      " 2024-08-02 18:37:19,854 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      " 2024-08-02 18:37:19,857 - DEBUG- close.started\n",
      " 2024-08-02 18:37:19,859 - DEBUG- close.complete\n",
      " 2024-08-02 18:37:19,861 - DEBUG- connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      " 2024-08-02 18:37:19,965 - DEBUG- connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C5A9E6C30>\n",
      " 2024-08-02 18:37:19,966 - DEBUG- start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023C63B4F6D0> server_hostname='api.openai.com' timeout=None\n",
      " 2024-08-02 18:37:20,004 - DEBUG- start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C63BCCC20>\n",
      " 2024-08-02 18:37:20,005 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 18:37:20,009 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 18:37:20,010 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 18:37:20,016 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 18:37:20,017 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 18:37:20,724 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 22:37:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'450'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199897'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'30ms'), (b'x-request-id', b'req_eac47d081ccff0e03b447781f8ed6101'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad18d49896ec936-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 18:37:20,725 - INFO- HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 18:37:20,727 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 18:37:20,729 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 18:37:20,730 - DEBUG- response_closed.started\n",
      " 2024-08-02 18:37:20,731 - DEBUG- response_closed.complete\n",
      " 2024-08-02 18:37:20,733 - DEBUG- HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 22:37:20 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '450', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199897', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '30ms', 'x-request-id': 'req_eac47d081ccff0e03b447781f8ed6101', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad18d49896ec936-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 18:37:20,735 - DEBUG- request_id: req_eac47d081ccff0e03b447781f8ed6101\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse LineList from completion 1. Got: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.8/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:26\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mBaseModel):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mBaseModel):\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\pydantic\\main.py:568\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[0;32m    567\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.8/v/model_type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is llama2?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:129\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    125\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    126\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    128\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:286\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[1;34m(self, llm_result)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m         {\n\u001b[1;32m--> 286\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    287\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[0;32m    288\u001b[0m         }\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[0;32m    290\u001b[0m     ]\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[0;32m    292\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:66\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:35\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     31\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model version for PydanticOutputParser: \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124m                    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m             )\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (pydantic\u001b[38;5;241m.\u001b[39mValidationError, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parser_exception(e, obj)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pydantic v1\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Failed to parse LineList from completion 1. Got: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.8/v/model_type"
     ]
    }
   ],
   "source": [
    "llm_chain.invoke(\"What is llama2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 18:25:25,843 - DEBUG- Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Your task is to generate 3 different search queries that aim to answer the user question from multiple perspetives.\\n            Each query MUST tackle the question from a different perspective. \\n            Provide these alternative questions separated by newlines.\\n            \\n            Original question:  Tell me about llama2?\\n            ', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'logprobs': False, 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      " 2024-08-02 18:25:25,846 - DEBUG- Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      " 2024-08-02 18:25:25,848 - DEBUG- close.started\n",
      " 2024-08-02 18:25:25,849 - DEBUG- close.complete\n",
      " 2024-08-02 18:25:25,851 - DEBUG- connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2024-08-02 18:25:25,945 - DEBUG- connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C63BC8500>\n",
      " 2024-08-02 18:25:25,946 - DEBUG- start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023C63B4F6D0> server_hostname='api.openai.com' timeout=None\n",
      " 2024-08-02 18:25:25,982 - DEBUG- start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C5A957A40>\n",
      " 2024-08-02 18:25:25,984 - DEBUG- send_request_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 18:25:25,986 - DEBUG- send_request_headers.complete\n",
      " 2024-08-02 18:25:25,988 - DEBUG- send_request_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 18:25:25,990 - DEBUG- send_request_body.complete\n",
      " 2024-08-02 18:25:25,992 - DEBUG- receive_response_headers.started request=<Request [b'POST']>\n",
      " 2024-08-02 18:25:26,920 - DEBUG- receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 02 Aug 2024 22:25:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'user-cgptltwq9ygldnaumgtpmpwd'), (b'openai-processing-ms', b'806'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199896'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'31ms'), (b'x-request-id', b'req_d9db0cc02d8c75d92cf64279b95c35c0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8ad17bdad977c5c7-IAD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      " 2024-08-02 18:25:26,921 - INFO- HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 2024-08-02 18:25:26,923 - DEBUG- receive_response_body.started request=<Request [b'POST']>\n",
      " 2024-08-02 18:25:26,925 - DEBUG- receive_response_body.complete\n",
      " 2024-08-02 18:25:26,926 - DEBUG- response_closed.started\n",
      " 2024-08-02 18:25:26,928 - DEBUG- response_closed.complete\n",
      " 2024-08-02 18:25:26,930 - DEBUG- HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 02 Aug 2024 22:25:27 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'user-cgptltwq9ygldnaumgtpmpwd', 'openai-processing-ms': '806', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199896', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '31ms', 'x-request-id': 'req_d9db0cc02d8c75d92cf64279b95c35c0', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8ad17bdad977c5c7-IAD', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      " 2024-08-02 18:25:26,931 - DEBUG- request_id: req_d9db0cc02d8c75d92cf64279b95c35c0\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse LineList from completion 1. Got: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.8/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:26\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mBaseModel):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mBaseModel):\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\pydantic\\main.py:568\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[0;32m    567\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.8/v/model_type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the query\u001b[39;00m\n\u001b[0;32m      2\u001b[0m retriever \u001b[38;5;241m=\u001b[39m MultiQueryRetriever(\n\u001b[0;32m      3\u001b[0m     retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[0;32m      4\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39m llm_chain,\n\u001b[0;32m      5\u001b[0m     parser_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mlen\u001b[39m(docs)\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     emit_warning()\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py:360\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_name:\n\u001b[0;32m    359\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[1;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py:221\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    224\u001b[0m         result,\n\u001b[0;32m    225\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py:214\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 214\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\retrievers\\multi_query.py:166\u001b[0m, in \u001b[0;36mMultiQueryRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    154\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    156\u001b[0m     run_manager: CallbackManagerForRetrieverRun,\n\u001b[0;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get relevant documents given a user query.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m        Unique union of relevant documents from all generated queries\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_original:\n\u001b[0;32m    168\u001b[0m         queries\u001b[38;5;241m.\u001b[39mappend(query)\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\retrievers\\multi_query.py:183\u001b[0m, in \u001b[0;36mMultiQueryRetriever.generate_queries\u001b[1;34m(self, question, run_manager)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_queries\u001b[39m(\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m    174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate queries based upon user input.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m        List of LLM generated queries that are similar to the user input\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain, LLMChain):\n\u001b[0;32m    187\u001b[0m         lines \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    161\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    162\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:129\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    125\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    126\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    128\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:286\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[1;34m(self, llm_result)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m         {\n\u001b[1;32m--> 286\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    287\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[0;32m    288\u001b[0m         }\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[0;32m    290\u001b[0m     ]\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[0;32m    292\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:66\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m json_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Maran\\StudyMaterials\\Git\\LangChain\\Multi-Query-RAG\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:35\u001b[0m, in \u001b[0;36mPydanticOutputParser._parse_obj\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     31\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported model version for PydanticOutputParser: \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124m                    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m             )\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (pydantic\u001b[38;5;241m.\u001b[39mValidationError, pydantic\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parser_exception(e, obj)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pydantic v1\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Failed to parse LineList from completion 1. Got: 1 validation error for LineList\n  Input should be a valid dictionary or instance of LineList [type=model_type, input_value=1, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.8/v/model_type"
     ]
    }
   ],
   "source": [
    "# Run the query\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever = vectorstore.as_retriever(),\n",
    "    llm_chain= llm_chain,\n",
    "    parser_key=\"lines\"\n",
    ")\n",
    "docs = retriever.get_relevant_documents(query=question)\n",
    "len(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
