{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Maran\\StudyMaterials\\Git\\LangChain\\LCEL\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Describe in 5 lines about the topic {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser)\n",
    "result = chain.invoke({\"topic\": \"AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'AI',\n",
       " 'text': 'Artificial Intelligence (AI) refers to the simulation of human intelligence in machines designed to think and learn like humans. It encompasses various subfields, including machine learning, natural language processing, and robotics. AI applications range from virtual assistants and recommendation systems to autonomous vehicles and medical diagnostics. The technology has the potential to revolutionize industries, improve efficiency, and enhance decision-making. However, it also raises ethical concerns regarding privacy, job displacement, and bias in algorithms.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can use the LCEL instead of LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser\n",
    "result = lcel_chain.invoke({\"topic\":\"Langchain Expression Language\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain Expression Language (LEL) is a powerful tool designed for building and managing complex workflows in natural language processing applications. It allows users to define expressions that can manipulate and transform data seamlessly. LEL integrates with various data sources and APIs, enabling dynamic interactions and real-time data processing. Its syntax is user-friendly, making it accessible for both developers and non-developers. Overall, LEL enhances the flexibility and efficiency of language model applications by streamlining the expression of logic and data handling.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Runnables in LCEL using the Vector DB in Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def get_vector_store(embedding_function=OpenAIEmbeddings(),\n",
    "                     storage_directory=\"embeddings\",\n",
    "                     collection_name=\"langchain\"):\n",
    "    vectordb = Chroma(\n",
    "        embedding_function=embedding_function,\n",
    "        persist_directory=storage_directory,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_a = get_vector_store(collection_name='vectorstore_a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a151ba2e-1ba2-4639-8074-3208908e46ec',\n",
       " 'f1e8de4e-df73-4ce3-9643-692e2d245cec',\n",
       " 'f81b3bd5-a900-4943-b673-cc91f0c230ad',\n",
       " '41ba7f74-78df-4fc8-92de-04c826a6304b']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_a.add_texts(['Langchain is a framework for developing GENAI Apps',\n",
    "                         'FastAPI helps to creates API application in python','Apple a day keeps the Doctor away',\n",
    "                         'Dan born in Oct 7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_b = get_vector_store(collection_name='vectorstore_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f8d612bf-3916-4f8a-94c3-2c5cc475b728',\n",
       " 'a06b84ed-db96-4998-b39b-90995e83046c',\n",
       " '83bc1dbc-f9c6-4be4-8873-877ea54f009e']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_b.add_texts(['Using LangChain we can easily connect to any Large Language Models',\n",
    "                         'Apple contains good source of nutrients',\n",
    "                         'Dan was born in 1983'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorstore_a.get()[\"ids\"]), len(vectorstore_b.get()[\"ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use the Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_a = vectorstore_a.as_retriever()\n",
    "retriever_b = vectorstore_b.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str=\"\"\"\n",
    "Answer the following question based on the provided context.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = RunnableParallel(\n",
    "    {\"context\":retriever_a,\"question\":RunnablePassthrough()}\n",
    "\n",
    ")\n",
    "chain = retrieval| prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain is a framework designed for developing Generative AI (GENAI) applications. It provides tools and components that facilitate the creation and deployment of applications that leverage generative AI technologies.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Explain about Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_prompt = \"\"\"\n",
    "Please answer the question based on the following contexts. If the context does not contain the answer, please say \"I don't know.\"\n",
    "Context:\n",
    "{context_a}\n",
    "\\n\\n\n",
    "{context_b}\n",
    "\\n\\n\n",
    "Question: {question}\n",
    "\\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(updated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_enhanced = RunnableParallel(\n",
    "    {\"context_a\":retriever_a,\n",
    "     \"context_b\":retriever_b,\n",
    "     \"question\":RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = retrieval_enhanced | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain is a framework for developing GENAI Apps.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Use of Langchain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "- This allows us convert and use our custom fuction in the Langchain Expression Language as a pipe operator.\n",
    "- Below is the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_output(data):\n",
    "    print(\"Thanks for using our AI chat\")\n",
    "    print(\"Here is the output follows: \")\n",
    "    print(\"\\n\\n\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_formatted_output = RunnableLambda(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me about the {Topic} in 2 lines\"\n",
    ")\n",
    "chain = prompt | llm | output_parser | add_formatted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for using our AI chat\n",
      "Here is the output follows: \n",
      "\n",
      "\n",
      "\n",
      "FastAPI is a modern, high-performance web framework for building APIs with Python 3.6+ based on standard Python type hints. It enables automatic generation of interactive API documentation and is designed for speed and ease of use, making it ideal for building RESTful services.\n"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"Python FastApi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
